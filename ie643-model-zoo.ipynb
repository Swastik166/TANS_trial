{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7046063,"sourceType":"datasetVersion","datasetId":4054560},{"sourceId":7051954,"sourceType":"datasetVersion","datasetId":4058600}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Swastik166/TANS_trial.git\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, './TANS_trial')\n!pip install config","metadata":{"execution":{"iopub.status.busy":"2023-11-25T18:21:43.473272Z","iopub.execute_input":"2023-11-25T18:21:43.473632Z","iopub.status.idle":"2023-11-25T18:21:55.468344Z","shell.execute_reply.started":"2023-11-25T18:21:43.473603Z","shell.execute_reply":"2023-11-25T18:21:55.466976Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting config\n  Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\nInstalling collected packages: config\nSuccessfully installed config-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"%%bash\ncd TANS_trial\npython3 main.py --gpu 0 --mode train --batch-size 140 --n-epochs 10000 --base-path path/for/storing/outcomes/ --data-path path/to/processed/dataset/is/stored/ --model-zoo path/to/model_zoo.pt --seed 777 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport os\nfrom PIL import Image\n\nbatch_size = 32\ndata_path = '/kaggle/input/ie-643-tans-train-sub-dataset'\n\n\n'''def get_loader(mode='train'):\n    dataset = ZooDatasets(mode=mode)\n    root_path = dataset.load_data()\n    loader = ImageFolder(root=root_path,transform=transforms.Compose([\n                                                  transforms.Resize((224, 224)),\n                                                  transforms.ToTensor(),\n                                              ]),\n                                              )\n    loader = DataLoader(dataset=loader,\n                        batch_size=batch_size,\n                        shuffle=(mode == 'train'),\n                        num_workers=4)\n    return dataset, loader'''\n\n\nclass ZooDatasets(Dataset):\n    \n    def __init__(self, mode='train', transform=None):\n        #self.args = args\n        self.mode = mode\n        self.transform = transform\n        self.data_path = data_path\n        self.dataset_list = [\n                'bottles',\n                'cassava_leaf_disease',\n                'casting products',\n                'corals',\n                'ct_images',\n                'four_shapes',\n                'fruits',\n                'lego_bricks',\n                'natural_images',\n                'store_items',\n        ]\n        \n        self.curr_dataset = self.dataset_list[0]\n        self.load_data()\n        \n        \n    def get_loader(self, mode='train'):\n        root_path = self.load_data()\n        loader = ImageFolder(root=root_path, transform=transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ]))\n\n        loader = DataLoader(\n            dataset=loader,\n            batch_size=batch_size,\n            shuffle=(mode == 'train'),\n            num_workers=4\n        )\n        \n        return loader\n        \n    def load_data(self):\n        if self.mode == 'train':\n            self.data_folder = os.path.join(self.data_path, f'{self.curr_dataset}/tr')\n        elif self.mode == 'validation':\n            self.data_folder = os.path.join(self.data_path, f'{self.curr_dataset}/va')\n        elif self.mode == 'test':\n            self.data_folder = os.path.join(self.data_path, f'{self.curr_dataset}/te')\n            \n            \n        self.classes = sorted([d.name for d in os.scandir(self.data_folder) if d.is_dir()])\n        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n        self.samples = []\n        #print(self.classes)\n        \n        for target_class in self.classes:\n            class_index = self.class_to_idx[target_class]\n            target_dir = os.path.join(self.data_folder, target_class)\n            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n                for fname in sorted(fnames):\n                    path = os.path.join(root, fname)\n                    self.samples.append((path, class_index))\n        \n        \n        return self.data_folder\n        \n     \n        \n    def get_dataset_list(self):\n        return self.dataset_list\n    \n    \n    def curr(self):\n        return self.curr_dataset\n    \n    def set_mode(self, mode):\n        self.mode = mode\n        \n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        img = Image.open(path).convert('RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, target\n    \n    \n    def set_dataset(self, dataset):\n        self.curr_dataset = dataset\n        if self.mode == 'train':\n            self.data_folder = os.path.join(self.data_path, f'{dataset}/tr')\n\n\n        elif self.mode == 'validation':\n            self.data_folder = os.path.join(self.data_path, f'{dataset}/va')\n\n            \n        elif self.mode == 'test':\n            self.data_folder = os.path.join(self.data_path, f'{dataset}/te')\n            \n    def get_nclss(self):\n        return (len(self.classes))\n    \n    def get_clss(self):\n        return (self.classes)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-25T19:59:12.775475Z","iopub.execute_input":"2023-11-25T19:59:12.776345Z","iopub.status.idle":"2023-11-25T19:59:17.965081Z","shell.execute_reply.started":"2023-11-25T19:59:12.776246Z","shell.execute_reply":"2023-11-25T19:59:17.963635Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tr_dataset = ZooDatasets(mode='train')\nte_dataset = ZooDatasets(mode='test')\nval_dataset = ZooDatasets(mode='validation')\n\n'''intr_loader = intr_dataset.get_loader(mode='train')\ninte_loader = inte_dataset.get_loader(mode='test')\ninval_loader = inval_dataset.get_loader(mode='validation')'''\n\nprint('outer', intr_dataset.curr_dataset)\n\nfor query_id, query_dataset in enumerate(intr_dataset.get_dataset_list()):\n    print('quesry_dataset',query_dataset)\n    tr_dataset.set_dataset(query_dataset)\n    te_dataset.set_dataset(query_dataset)\n    val_dataset.set_dataset(query_dataset)\n    \n    tr_loader = intr_dataset.get_loader(mode='train')\n    te_loader = inte_dataset.get_loader(mode='test')\n    val_loader = inval_dataset.get_loader(mode='validation')\n    \n    \n    print(tr_dataset.curr_dataset)\n    x, y = next(iter(tr_loader))\n    print('y for tr', y)\n    x, y = next(iter(te_loader))\n    print('y for te', y)\n    \n    clss = tr_dataset.get_clss()\n    nclss = tr_dataset.get_nclss()\n    \n    print('nclass', nclss )\n    print('clss', clss )\n    \n\n    print('tr_loader', len(tr_loader)*batch_size)\n    print('te_loader', len(te_loader)*batch_size)\n    print('val_loader', len(val_loader)*batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-25T19:59:39.106501Z","iopub.execute_input":"2023-11-25T19:59:39.107045Z","iopub.status.idle":"2023-11-25T19:59:39.760414Z","shell.execute_reply.started":"2023-11-25T19:59:39.107014Z","shell.execute_reply":"2023-11-25T19:59:39.758370Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mZooDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m te_dataset \u001b[38;5;241m=\u001b[39m ZooDatasets(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ZooDatasets(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36mZooDatasets.__init__\u001b[0;34m(self, mode, transform)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottles\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcassava_leaf_disease\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_items\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     48\u001b[0m ]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mZooDatasets.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/te\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([d\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_folder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mis_dir()])\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: i \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)}\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m []\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/ie-643-tans-train-sub-dataset/bottles/tr'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/ie-643-tans-train-sub-dataset/bottles/tr'","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport torchvision.models as models\nfrom tqdm import tqdm\n\n\"\"\"add utils from misc and use the finctions available in for better error handling\"\"\"\n\nclass ModelZoo:\n    def __init__(self, datasets):\n        self.seed = 777\n        self.datasets = datasets\n        self.models = {}\n        self.train_instances = {}\n        self.noise = torch.load('/path/to/noise.pt')\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        torch.cuda.manual_seed(self.seed)\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n        random.seed(self.seed)\n\n\n        \n        \n    def init_loaders(self, dataset):\n        # Get loaders for train, test, and validation data\n        print('=====> LOADING dataset and loaders for train, test and validation <=====')\n        MetaTestDataset.set_dataset()\n        self.tr_dataset, self.tr_loader = get_loader(dataset, mode = 'train')\n        self.te_dataset, self.te_loader = get_loader(dataset, mode = 'test')\n        self.val_dataset, self.val_loader = get_loader(dataset, mode = 'validation')\n        self.nclass = self.tr_dataset.get_nclss()\n        \n        \n        \n        \n    def create_zoo(self):\n        zoo = {}\n\n        for dataset in self.datasets:\n            # Initializing loaders for the dataset\n            self.init_loaders(dataset)\n\n\n            for _ in range(10):\n                # Get neural network model and topology information\n                print(f'=====> Generating {_}th   <=====')\n                topol, net = self.get_net(nclass)\n\n                # Training the model and obtaining accuracy\n                lss = torch.nn.CrossEntropyLoss()\n                optim = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=4e-5)\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim , float(self.args.n_eps_finetuning))\n                \n                acc = self.train(net, dataset, tr_loader, val_loader, nclass)  # Assuming train method returns accuracy\n\n                # Calculating number of parameters\n                n_params = self.n_param(net)\n                f_emb = self.f_emb(net)\n\n                \n                del net\n                del optim\n                del lss\n                \n                zoo['dataset'].append(dataset)\n                zoo['topol'].append(topol)\n                zoo['acc'].append(acc)\n                zoo['f_emb'].append(f_emb)\n                zoo['n_params'].append(n_params)\n                \n                x_query_train, x_query_test = self.get_query(tr_loader, te_loader)\n                self.save_trainpt(self, dataset, nclss)\n                \n        self.save_zoo(zoo)\n        \n                \n                \n\n\n    def get_net(self, nclss):\n        super_net_name = \"ofa_supernet_mbv3_w10\"\n\n        # Load the super network from the 'mit-han-lab/once-for-all' repository\n        super_net = torch.hub.load('mit-han-lab/once-for-all', super_net_name, pretrained=True).eval()\n\n        # Sample an active subnet configuration from the super network\n        sampled_config = super_net.sample_active_subnet()\n\n        # Extract topology information from the sampled configuration\n        pre_topol = list(sampled_config.values())\n        topol = []\n\n        for i in pre_topol:\n            for j in i:\n                topol.append(j)\n\n        # Split the topology into kernel sizes, expansion ratios, and depths\n        ks = topol[:20] \n        e = topol[20:40]\n        d = topol[40:]\n\n        # Set the active subnet in the super network using the sampled topology\n        super_net.set_active_subnet(ks=ks, e=e, d=d)\n        active_subnet = super_net.get_active_subnet(preserve_weight=True)\n        active_subnet.classifier = torch.nn.Linear(1536, nclss)\n        active_subnet = active_subnet.to(self.device)\n\n        return topol, active_subnet\n\n    \n    \n    \n    def f_emb(self, net):\n        model = net        \n        module = list(model.children())[:-1]\n        model = torch.nn.Sequential(*module)\n        model.eval()\n        with torch.no_grad():\n            # Forward pass through ResNet18\n            f_emb = model(self.noise)\n        print(f_emb)\n        return f_emb\n\n    \n    \n    \n\n    def train(self, model, dataset, train_loader, val_loader, nclss):\n        self.model = model\n        print(f'Starting Training for:{dataset} with model topology:{self.topol}')\n        lr = lr\n        counter = 0\n        best_val_loss = 10000\n        val_acc = list()\n        \n        \n        for ep in range(self.args.n_eps_finetuning):\n            self.curr_ep = ep\n            ep_loss_tr = 0.0\n            ep_loss_val = 0.0\n            ep_tr_time = 0\n            \n            \n            self.model.train()\n            for b_id, batch in tqdm(enumerate(self.tr_loader)):\n                self.optim.zero_grad()\n                st = time.time()\n                x,y = batch\n                output = self.model(x.to(self.device))\n                loss = self.lss(output, y.to(self.device))\n                loss.backward()\n                self.optim.step()\n                self.scheduler.step()\n                        \n                tr_loss = loss.item()\n                        \n                ep_loss_tr += tr_loss * x.size(0)\n                \n            ep_loss_tr = ep_loss_tr/len(self.tr_loader)\n\n                        \n            self.model.eval()\n            total_val = 0\n            crrct_val = 0\n                        \n                        \n            for v_id, (x,y) in tqdm(enumerate(self.val_loader)):\n                outputs = self.model(x.to(self.device))\n                loss_v = self.lss(outputs, y.to(self.device))\n                       \n                val_loss = loss_v.detach.item()\n                \n                ep_loss_val += val_loss * x.size(0)\n                        \n                pred = torch.argmax(outputs, dim = 1)\n                total_val += y.size(0)\n                correct_val += (pred == y.to(self.device)).sum().item() \n                        \n            acc = (100*correct_val)/total_val\n            val_acc.append(acc)\n            \n            ep_loss_val = ep_loss_val/len(self.val_loader)\n    \n            if ep_loss_val < best_val_loss:\n                best_val_loss = ep_loss_tr\n                counter = 0\n                \n            elif ep_loss_val > min_loss:\n                counter += 1\n                \n            if counter >= patience:\n                print(f\"Early stopping on, {ep}th, epoch\")\n                break\n                \n        return val_acc[-1]\n            \n            \n                \n         \n            \n        dura = time.time() -st\n        print(\n        f' ==> [dataset:{self.dataset}]'+\n        f' ep:{ep+1}, tr_lss:{tr_lss:.3f}, accuracy:{}'+\n        f' val_lss:{val_lss:.3f}, val_acc: {val_acc:.3f},'+\n        f' tr_time:{ep_tr_time:.3f}s, val_time:{ep_val_time:.3f}s')\n            \n                        \n                        \n                        \n                        \n    \n    def n_param(self, model):\n        # Calculate the number of parameters in the model\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    \n\n    def get_query(self, train_loader, test_loader):\n        model = models.resneth18(pretrained = True)\n        module = list(model.children())[:-1]\n        model = torch.nn.Sequential(*module)\n        model.eval()\n        with torch.no_grad():\n            images, _ =  next(iter(train_loader))\n            print(len(images))\n            # Forward pass through ResNet18\n            query_train = model(images)\n        print (query_train.shape)\n        with torch.no_grad():\n            images, _ =  next(iter(test_loader))\n            print(len(images))\n            # Forward pass through ResNet18\n        query_test = model(images)\n        print (query_test.shape)\n        return query_train, query_test\n    \n    \n    \n\n    def save_zoo(self, dict):\n        # Save model zoo to model_zoo.pt\n\n        torch.save(dict, 'model_zoo.pt')\n\n        \n        \n    def save_trainpt(self):\n        # Save training instances to m_train.pt\n        trainpt_dict = {}\n        for dataset in self.datasets:\n            trainpt_dict[dataset] = {\n                'task': {},  # Placeholder for task information\n                'clss': {},  # Placeholder for class information\n                'nclss': {},  # Placeholder for number of classes\n                'x_query_train': {},  # Placeholder for training query data\n                'x_query_test': {}  # Placeholder for testing query data\n            }\n        torch.save(trainpt_dict, 'm_train.pt')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_zoo = {'dataset':['dataset1', 'dataset1', 'dataset2', 'dataset2', 'dataset3', 'dataset3'],\n             'topol':['model1', 'model2', 'model3', 'model4' , 'model5',  'model6'],\n             'acc':['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6'],\n             'f_emb':['f_emb1', 'f_emb2', 'f_emb3', 'f_emb4', 'f_emb5', 'f_emb6'],\n             'n_params':['np1', 'np2', 'np3', 'np4', 'np5', 'np6']}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_zoo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \ntorch.save(model_zoo, 'zoo.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zoo = torch.load('zoo.pt')\nprint(zoo)\nprint(type(zoo))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zoo['dataset'].append('test')\nzoo['topol'].append('test')\nzoo['acc'].append('test')\nzoo['f_emb'].append('test')\nzoo['n_params'].append('test')\nprint(zoo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}